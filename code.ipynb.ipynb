{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13648470,"sourceType":"datasetVersion","datasetId":8676557}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom catboost import CatBoostClassifier, Pool\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier, early_stopping, log_evaluation\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder, PowerTransformer, StandardScaler, QuantileTransformer\nfrom scipy.stats import rankdata\nfrom scipy.optimize import minimize\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"=\"*80)\nprint(\"ðŸš€ OPTIMIZED WINNING SCRIPT (v12.0 - TARGET: 0.69+ ROC) ðŸš€\")\nprint(\"=\"*80)\nprint(\"Focus: BEST FEATURES ONLY | Enhanced Engineering | Optimized Models\")\nprint(\"=\"*80)\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\nN_SPLITS = 7  # Increased for more stable CV\nRANDOM_STATE = 42\nDATA_PATH = \"/kaggle/input/dataset-ai108/\"\nUSE_GPU = True\n\n# ============================================================================\n# STEP 1: LOAD DATA\n# ============================================================================\nprint(\"\\n[1/9] Loading data...\")\ntry:\n    train_df = pd.read_csv(f\"{DATA_PATH}train.csv\")\n    test_df = pd.read_csv(f\"{DATA_PATH}test.csv\")\n    print(f\"âœ“ Train: {train_df.shape} | Test: {test_df.shape}\")\n    print(f\"âœ“ Default rate: {train_df['Default 12 Flag'].mean():.4f}\")\nexcept FileNotFoundError:\n    print(f\"âŒ ERROR: Files not found at {DATA_PATH}\")\n    raise\n\n# ============================================================================\n# STEP 2: ELITE FEATURE ENGINEERING - ONLY THE BEST\n# ============================================================================\nprint(\"\\n[2/9] Creating ELITE features (quality > quantity)...\")\n\ndef create_elite_features(df):\n    \"\"\"Focus on the most predictive features only\"\"\"\n    df = df.copy()\n    \n    # === CORE TEMPORAL FEATURES ===\n    df['Application Date'] = pd.to_datetime(df['Application Date'], format='%Y/%m/%d', errors='coerce')\n    df['Date of Birth'] = pd.to_datetime(df['Date of Birth'], format='%Y/%m/%d', errors='coerce')\n    \n    # Age - CRITICAL FEATURE\n    df['Age'] = (df['Application Date'] - df['Date of Birth']).dt.days / 365.25\n    df['Age_Squared'] = df['Age'] ** 2\n    df['Age_Cubed'] = df['Age'] ** 3\n    \n    # Time features - selective\n    df['App_Month'] = df['Application Date'].dt.month\n    df['App_DayOfWeek'] = df['Application Date'].dt.dayofweek\n    df['App_Quarter'] = df['Application Date'].dt.quarter\n    df['App_Day'] = df['Application Date'].dt.day\n    \n    df['App_Hour'] = df['Application Time'] // 10000\n    df['App_Minute'] = (df['Application Time'] % 10000) // 100\n    \n    # Elite time patterns\n    df['Is_Weekend'] = (df['App_DayOfWeek'] >= 5).astype(int)\n    df['Is_LateNight'] = ((df['App_Hour'] >= 22) | (df['App_Hour'] <= 5)).astype(int)\n    df['Is_BusinessHours'] = ((df['App_Hour'] >= 9) & (df['App_Hour'] <= 17)).astype(int)\n    df['Is_MonthEnd'] = (df['App_Day'] >= 25).astype(int)\n    \n    # === HONESTY SCORE - TOP PREDICTOR ===\n    df['Loan_Amount_Gap'] = df['Declared Amount of Unsecured Loans'] - df['Amount of Unsecured Loans']\n    df['Loan_Count_Gap'] = df['Declared Number of Unsecured Loans'] - df['Number of Unsecured Loans']\n    df['Abs_Amount_Gap'] = abs(df['Loan_Amount_Gap'])\n    df['Abs_Count_Gap'] = abs(df['Loan_Count_Gap'])\n    \n    # Enhanced honesty metrics\n    df['Honesty_Score'] = np.where(\n        df['Declared Amount of Unsecured Loans'] > 0,\n        1 - np.clip(df['Abs_Amount_Gap'] / (df['Declared Amount of Unsecured Loans'] + 1), 0, 1),\n        1\n    )\n    \n    df['Amount_Gap_Pct'] = np.where(\n        df['Amount of Unsecured Loans'] > 0,\n        df['Loan_Amount_Gap'] / (df['Amount of Unsecured Loans'] + 1),\n        0\n    )\n    \n    df['Perfect_Honesty'] = ((df['Loan_Amount_Gap'] == 0) & (df['Loan_Count_Gap'] == 0)).astype(int)\n    df['Severe_Dishonesty'] = ((df['Loan_Amount_Gap'] < -100000) | (df['Loan_Count_Gap'] < -1)).astype(int)\n    df['Mild_Dishonesty'] = ((df['Abs_Amount_Gap'] > 0) & (df['Abs_Amount_Gap'] <= 100000)).astype(int)\n    \n    # === DTI RATIOS - CRITICAL PREDICTORS ===\n    df['Income_Safe'] = df['Total Annual Income'] + 1  # Avoid division by zero\n    \n    # Core DTI calculations\n    df['DTI_Total'] = (df['Amount of Unsecured Loans'] + df['Application Limit Amount(Desired)']) / df['Income_Safe']\n    df['DTI_Current'] = df['Amount of Unsecured Loans'] / df['Income_Safe']\n    df['DTI_Desired'] = df['Application Limit Amount(Desired)'] / df['Income_Safe']\n    \n    # DTI with housing costs\n    df['DTI_WithRent'] = (df['Amount of Unsecured Loans'] + df['Rent Burden Amount'] * 12) / df['Income_Safe']\n    df['DTI_Full_Burden'] = (df['Amount of Unsecured Loans'] + df['Application Limit Amount(Desired)'] + df['Rent Burden Amount'] * 12) / df['Income_Safe']\n    \n    # DTI risk flags\n    df['DTI_Critical'] = (df['DTI_Total'] > 0.6).astype(int)\n    df['DTI_High'] = ((df['DTI_Total'] > 0.4) & (df['DTI_Total'] <= 0.6)).astype(int)\n    df['DTI_Safe'] = (df['DTI_Total'] <= 0.2).astype(int)\n    \n    # === INCOME ADEQUACY - TOP PREDICTOR ===\n    df['Income_per_Dependent'] = df['Total Annual Income'] / (df['Number of Dependents'] + 1)\n    df['Income_per_Person'] = df['Total Annual Income'] / (df['Number of Dependents'] + 2)\n    df['Monthly_Income'] = df['Total Annual Income'] / 12\n    df['Monthly_Disposable'] = (df['Total Annual Income'] / 12) - df['Rent Burden Amount']\n    df['Disposable_After_Loans'] = df['Monthly_Disposable'] - (df['Amount of Unsecured Loans'] / 120)  # Assume 10-year repayment\n    \n    # === LOAN CHARACTERISTICS ===\n    df['Avg_Existing_Loan'] = df['Amount of Unsecured Loans'] / (df['Number of Unsecured Loans'] + 1)\n    df['Loan_Request_Ratio'] = df['Application Limit Amount(Desired)'] / (df['Avg_Existing_Loan'] + 1)\n    df['Loan_Intensity'] = df['Number of Unsecured Loans'] / (df['Age'] + 1)\n    df['Total_Debt_Exposure'] = df['Amount of Unsecured Loans'] + df['Application Limit Amount(Desired)']\n    \n    # Loan risk flags\n    df['Has_Many_Loans'] = (df['Number of Unsecured Loans'] >= 3).astype(int)\n    df['Has_Extreme_Loans'] = (df['Number of Unsecured Loans'] >= 5).astype(int)\n    df['Loan_Free'] = (df['Number of Unsecured Loans'] == 0).astype(int)\n    df['Single_Loan'] = (df['Number of Unsecured Loans'] == 1).astype(int)\n    \n    # === STABILITY INDICATORS ===\n    df['Employment_Years'] = df['Duration of Employment at Company (Months)'] / 12\n    df['Residence_Years'] = df['Duration of Residence (Months)'] / 12\n    \n    df['Job_Stability'] = df['Employment_Years'] / (df['Age'] + 1)\n    df['Residence_Stability'] = df['Residence_Years'] / (df['Age'] + 1)\n    df['Combined_Stability'] = (df['Employment_Years'] + df['Residence_Years']) / 2\n    df['Stability_Product'] = np.sqrt(df['Employment_Years'] * df['Residence_Years'])  # Geometric mean\n    \n    # Stability flags\n    df['Is_Job_Hopper'] = (df['Employment_Years'] < 1).astype(int)\n    df['Is_Veteran_Employee'] = (df['Employment_Years'] >= 10).astype(int)\n    df['Is_New_Resident'] = (df['Residence_Years'] < 2).astype(int)\n    df['Is_Stable'] = ((df['Employment_Years'] >= 3) & (df['Residence_Years'] >= 3)).astype(int)\n    \n    # === HOUSING & RENT BURDEN ===\n    df['Housing_Burden'] = df['Rent Burden Amount'] / (df['Monthly_Income'] + 1)\n    df['Is_Homeowner'] = df['Residence Type'].isin([1, 2, 8]).astype(int)\n    df['Is_Renter'] = df['Residence Type'].isin([4, 5, 6, 7]).astype(int)\n    df['High_Rent_Burden'] = (df['Housing_Burden'] > 0.3).astype(int)\n    \n    # === EMPLOYMENT QUALITY ===\n    df['Is_Regular_Employee'] = (df['Employment Status Type'] == 1).astype(int)\n    df['Is_Large_Company'] = df['Company Size Category'].isin([1, 2, 3, 4]).astype(int)\n    df['Is_Small_Company'] = df['Company Size Category'].isin([8, 9, 10]).astype(int)\n    df['Is_Part_Time'] = (df['Employment Type'] == 4).astype(int)\n    df['Is_Self_Employed'] = (df['Employment Type'] == 3).astype(int)\n    \n    # === FAMILY STRUCTURE ===\n    df['Is_Married'] = (df['Single/Married Status'] == 2).astype(int)\n    df['Has_Children'] = (df['Number of Dependent Children'] > 0).astype(int)\n    df['Large_Family'] = (df['Number of Dependents'] >= 3).astype(int)\n    df['Single_Parent'] = ((df['Single/Married Status'] != 2) & (df['Number of Dependents'] > 0)).astype(int)\n    df['Family_Pressure'] = df['Number of Dependents'] / (df['Income_Safe'] / 1000000)\n    \n    # === AGE CATEGORIES ===\n    df['Is_Young'] = (df['Age'] < 30).astype(int)\n    df['Is_Prime_Age'] = ((df['Age'] >= 30) & (df['Age'] < 50)).astype(int)\n    df['Is_Senior'] = (df['Age'] >= 55).astype(int)\n    df['Career_Start'] = ((df['Age'] >= 22) & (df['Age'] <= 28)).astype(int)\n    df['Peak_Earning'] = ((df['Age'] >= 40) & (df['Age'] <= 55)).astype(int)\n    \n    # === LOG TRANSFORMATIONS ===\n    df['Income_log'] = np.log1p(df['Total Annual Income'])\n    df['Existing_Loan_log'] = np.log1p(df['Amount of Unsecured Loans'])\n    df['Desired_Loan_log'] = np.log1p(df['Application Limit Amount(Desired)'])\n    df['Rent_log'] = np.log1p(df['Rent Burden Amount'])\n    df['Age_log'] = np.log1p(df['Age'])\n    \n    # === ELITE INTERACTION FEATURES (PROVEN PREDICTORS) ===\n    df['Age_Income'] = df['Age'] * df['Income_log']\n    df['Age_DTI'] = df['Age'] * df['DTI_Total']\n    df['Honesty_DTI'] = df['Honesty_Score'] * df['DTI_Total']\n    df['Honesty_Income'] = df['Honesty_Score'] * df['Income_log']\n    df['Stability_Income'] = df['Combined_Stability'] * df['Income_log']\n    df['Age_Stability'] = df['Age'] * df['Combined_Stability']\n    df['Income_Dependents'] = df['Income_log'] * (df['Number of Dependents'] + 1)\n    \n    # Triple interactions (most powerful)\n    df['Elite_Risk_1'] = df['Age'] * df['DTI_Total'] * df['Honesty_Score']\n    df['Elite_Risk_2'] = df['Income_log'] * df['Combined_Stability'] * (1 - df['DTI_Total'])\n    df['Elite_Risk_3'] = df['Age'] * df['Income_per_Dependent'] / (df['DTI_Total'] + 0.1)\n    \n    # === COMPOSITE RISK SCORES (CAREFULLY WEIGHTED) ===\n    df['Financial_Risk'] = (\n        df['DTI_Critical'] * 5 +\n        df['DTI_High'] * 3 +\n        df['Has_Many_Loans'] * 3 +\n        df['Has_Extreme_Loans'] * 4 +\n        df['Severe_Dishonesty'] * 5 +\n        df['High_Rent_Burden'] * 2\n    )\n    \n    df['Stability_Risk'] = (\n        df['Is_Job_Hopper'] * 3 +\n        df['Is_Part_Time'] * 2 +\n        df['Is_New_Resident'] * 2 +\n        (1 - df['Is_Homeowner']) * 1 +\n        df['Single_Parent'] * 3 +\n        (1 - df['Is_Large_Company']) * 1\n    )\n    \n    df['Demographic_Risk'] = (\n        df['Is_Young'] * 2 +\n        df['Is_Senior'] * 2 +\n        df['Large_Family'] * 2 +\n        (1 - df['Is_Married']) * 1\n    )\n    \n    df['Total_Risk_Score'] = df['Financial_Risk'] + df['Stability_Risk'] + df['Demographic_Risk']\n    \n    # Protective factors\n    df['Protective_Score'] = (\n        df['Is_Stable'] * 3 +\n        df['Is_Veteran_Employee'] * 2 +\n        df['Is_Large_Company'] * 2 +\n        df['DTI_Safe'] * 2 +\n        df['Perfect_Honesty'] * 3 +\n        df['Peak_Earning'] * 2 +\n        df['Is_Homeowner'] * 2\n    )\n    \n    df['Net_Risk_Score'] = df['Total_Risk_Score'] - df['Protective_Score']\n    \n    # === JIS ADDRESS FEATURES (LOCATION) ===\n    df['JIS_str'] = df['JIS Address Code'].fillna(-999).astype(str)\n    df['JIS_Prefix_2'] = df['JIS_str'].str[:2]\n    df['JIS_Prefix_3'] = df['JIS_str'].str[:3]\n    \n    # === RATIO FEATURES (HIGHLY PREDICTIVE) ===\n    df['Desired_to_Income'] = df['Application Limit Amount(Desired)'] / df['Income_Safe']\n    df['Existing_to_Income'] = df['Amount of Unsecured Loans'] / df['Income_Safe']\n    df['Rent_to_Income'] = (df['Rent Burden Amount'] * 12) / df['Income_Safe']\n    df['Total_Obligation_Ratio'] = (df['Total_Debt_Exposure'] + df['Rent Burden Amount'] * 12) / df['Income_Safe']\n    \n    # === PERCENTILE FEATURES (RELATIVE RANKINGS) ===\n    df['Income_Rank'] = rankdata(df['Total Annual Income']) / len(df)\n    df['Age_Rank'] = rankdata(df['Age']) / len(df)\n    df['DTI_Rank'] = rankdata(df['DTI_Total']) / len(df)\n    df['Stability_Rank'] = rankdata(df['Combined_Stability']) / len(df)\n    \n    # Drop temporary columns\n    df = df.drop(columns=['Application Date', 'Date of Birth', 'JIS_str', 'Income_Safe'], errors='ignore')\n    \n    return df\n\n# Apply features\ntrain_features = create_elite_features(train_df)\ntest_features = create_elite_features(test_df)\n\nprint(f\"âœ“ Created {train_features.shape[1] - train_df.shape[1]} elite features\")\nprint(f\"âœ“ Total features: {train_features.shape[1]}\")\n\n# ============================================================================\n# STEP 3: ADVERSARIAL VALIDATION WITH ELITE FEATURES\n# ============================================================================\nprint(\"\\n[3/9] Adversarial Validation...\")\nav_features = ['Age', 'DTI_Total', 'Honesty_Score', 'Combined_Stability', \n               'Income_per_Dependent', 'App_Month', 'Total_Risk_Score']\nav_features = [f for f in av_features if f in train_features.columns]\n\nav_X = pd.concat([train_features[av_features], test_features[av_features]], axis=0, ignore_index=True)\nav_y = np.array([0] * len(train_features) + [1] * len(test_features))\nav_X = av_X.fillna(-999)\n\nav_model = LGBMClassifier(n_estimators=500, learning_rate=0.05, max_depth=5, n_jobs=-1, verbosity=-1)\nav_model.fit(av_X, av_y)\nav_preds = av_model.predict_proba(av_X)[:, 1]\nav_auc = roc_auc_score(av_y, av_preds)\nprint(f\"âœ“ Adversarial AUC: {av_auc:.5f} (lower is better)\")\n\ntrain_features['av_score'] = av_preds[:len(train_features)]\ntest_features['av_score'] = av_preds[len(train_features):]\n\n# ============================================================================\n# STEP 4: PREPARE DATA\n# ============================================================================\nprint(\"\\n[4/9] Preparing data...\")\n\ny = train_features['Default 12 Flag']\nX = train_features.drop(columns=['Default 12 Flag', 'ID'], errors='ignore')\n\ntest_ids = test_features['ID']\nX_test = test_features.drop(columns=['ID'], errors='ignore')\nX_test = X_test.reindex(columns=X.columns, fill_value=0)\n\n# Define categorical features\ncat_features = [\n    'Major Media Code', 'Internet Details', 'Reception Type Category',\n    'Gender', 'Single/Married Status', 'Residence Type', 'Name Type',\n    'Family Composition Type', 'Living Arrangement Type',\n    'Insurance Job Type', 'Employment Type', 'Employment Status Type',\n    'Industry Type', 'Company Size Category', 'JIS Address Code',\n    'App_Month', 'App_DayOfWeek', 'App_Quarter',\n    'JIS_Prefix_2', 'JIS_Prefix_3'\n]\ncat_features = [col for col in cat_features if col in X.columns]\n\n# Handle missing values\nfor col in cat_features:\n    X[col] = X[col].fillna(-999).astype(str)\n    X_test[col] = X_test[col].fillna(-999).astype(str)\n\nnumeric_cols = [col for col in X.columns if col not in cat_features]\nX[numeric_cols] = X[numeric_cols].fillna(-999)\nX_test[numeric_cols] = X_test[numeric_cols].fillna(-999)\n\nX = X.replace([np.inf, -np.inf], -999)\nX_test = X_test.replace([np.inf, -np.inf], -999)\n\nprint(f\"âœ“ Features: {X.shape[1]} | Categorical: {len(cat_features)}\")\n\n# ============================================================================\n# STEP 5: ADVANCED PREPROCESSING\n# ============================================================================\nprint(\"\\n[5/9] Advanced preprocessing...\")\n\n# Power transform skewed features\nskewed_features = ['Total Annual Income', 'Amount of Unsecured Loans', \n                   'Application Limit Amount(Desired)', 'Rent Burden Amount']\nskewed_features = [f for f in skewed_features if f in numeric_cols]\n\npt = QuantileTransformer(output_distribution='normal', n_quantiles=1000)\nif len(skewed_features) > 0:\n    X_skewed = X[skewed_features].replace(-999, 0)\n    X_test_skewed = X_test[skewed_features].replace(-999, 0)\n    \n    X_transformed = pt.fit_transform(X_skewed)\n    X_test_transformed = pt.transform(X_test_skewed)\n    \n    for i, col in enumerate(skewed_features):\n        X[f'{col}_quantile'] = X_transformed[:, i]\n        X_test[f'{col}_quantile'] = X_test_transformed[:, i]\n    \n    numeric_cols.extend([f'{col}_quantile' for col in skewed_features])\n    print(f\"  âœ“ Quantile transformed {len(skewed_features)} features\")\n\n# Standard scale key features\nscaler = StandardScaler()\nscale_features = ['Age', 'Total_Risk_Score', 'Net_Risk_Score', 'DTI_Total', 'Combined_Stability']\nscale_features = [f for f in scale_features if f in numeric_cols]\n\nif len(scale_features) > 0:\n    X_scale = X[scale_features].replace(-999, 0)\n    X_test_scale = X_test[scale_features].replace(-999, 0)\n    \n    X_scaled = scaler.fit_transform(X_scale)\n    X_test_scaled = scaler.transform(X_test_scale)\n    \n    for i, col in enumerate(scale_features):\n        X[f'{col}_scaled'] = X_scaled[:, i]\n        X_test[f'{col}_scaled'] = X_test_scaled[:, i]\n    \n    numeric_cols.extend([f'{col}_scaled' for col in scale_features])\n    print(f\"  âœ“ Scaled {len(scale_features)} features\")\n\n# ============================================================================\n# STEP 6: LABEL ENCODERS\n# ============================================================================\nprint(\"\\n[6/9] Encoding categorical features...\")\nlabel_encoders = {}\nfor col in cat_features:\n    le = LabelEncoder()\n    all_cats = pd.concat([X[col], X_test[col]]).unique()\n    le.fit(all_cats)\n    label_encoders[col] = le\n\n# ============================================================================\n# STEP 7: OPTIMIZED MODEL TRAINING\n# ============================================================================\nprint(f\"\\n[7/9] Training elite models with {N_SPLITS}-Fold CV...\")\n\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n\ncb_oof = np.zeros(len(X))\nxgb_oof = np.zeros(len(X))\nlgb_oof = np.zeros(len(X))\ncb_test = np.zeros(len(X_test))\nxgb_test = np.zeros(len(X_test))\nlgb_test = np.zeros(len(X_test))\n\ncb_scores = []\nxgb_scores = []\nlgb_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f\"\\n{'='*70}\\nFOLD {fold+1}/{N_SPLITS}\\n{'='*70}\")\n    \n    X_tr, y_tr = X.iloc[train_idx].copy(), y.iloc[train_idx].copy()\n    X_val, y_val = X.iloc[val_idx].copy(), y.iloc[val_idx].copy()\n    \n    # --- CATBOOST (CHAMPION MODEL) ---\n    print(\"â†’ [1/3] CatBoost...\")\n    train_pool = Pool(X_tr, y_tr, cat_features=cat_features)\n    val_pool = Pool(X_val, y_val, cat_features=cat_features)\n    \n    cb_params = {\n        'iterations': 10000,\n        'learning_rate': 0.015,\n        'depth': 10,\n        'l2_leaf_reg': 15,\n        'min_data_in_leaf': 20,\n        'border_count': 254,\n        'feature_border_type': 'Median',\n        'eval_metric': 'AUC',\n        'random_seed': RANDOM_STATE + fold,\n        'early_stopping_rounds': 400,\n        'verbose': 0,\n        'thread_count': -1,\n        'boosting_type': 'Plain'\n    }\n    if USE_GPU:\n        cb_params['task_type'] = 'GPU'\n    \n    cb = CatBoostClassifier(**cb_params)\n    cb.fit(train_pool, eval_set=val_pool)\n    \n    cb_oof[val_idx] = cb.predict_proba(X_val)[:, 1]\n    cb_test += cb.predict_proba(X_test)[:, 1] / N_SPLITS\n    cb_score = roc_auc_score(y_val, cb_oof[val_idx])\n    cb_scores.append(cb_score)\n    print(f\"  âœ“ AUC: {cb_score:.6f}\")\n    \n    # --- XGBOOST ---\n    print(\"â†’ [2/3] XGBoost...\")\n    X_tr_xgb, X_val_xgb, X_test_xgb = X_tr.copy(), X_val.copy(), X_test.copy()\n    for col in cat_features:\n        X_tr_xgb[col] = label_encoders[col].transform(X_tr_xgb[col])\n        X_val_xgb[col] = label_encoders[col].transform(X_val_xgb[col])\n        X_test_xgb[col] = label_encoders[col].transform(X_test_xgb[col])\n    \n    xgb_params = {\n        'n_estimators': 10000,\n        'learning_rate': 0.015,\n        'max_depth': 9,\n        'min_child_weight': 10,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'colsample_bylevel': 0.7,\n        'reg_alpha': 1.0,\n        'reg_lambda': 1.0,\n        'gamma': 0.1,\n        'random_state': RANDOM_STATE + fold,\n        'eval_metric': 'auc',\n        'early_stopping_rounds': 400,\n        'n_jobs': -1,\n        'verbosity': 0\n    }\n    if USE_GPU:\n        xgb_params['tree_method'] = 'gpu_hist'\n        xgb_params['predictor'] = 'gpu_predictor'\n    \n    xgb = XGBClassifier(**xgb_params)\n    xgb.fit(X_tr_xgb, y_tr, eval_set=[(X_val_xgb, y_val)], verbose=False)\n    \n    xgb_oof[val_idx] = xgb.predict_proba(X_val_xgb)[:, 1]\n    xgb_test += xgb.predict_proba(X_test_xgb)[:, 1] / N_SPLITS\n    xgb_score = roc_auc_score(y_val, xgb_oof[val_idx])\n    xgb_scores.append(xgb_score)\n    print(f\"  âœ“ AUC: {xgb_score:.6f}\")\n    \n    # --- LIGHTGBM ---\n    print(\"â†’ [3/3] LightGBM...\")\n    X_tr_lgb, X_val_lgb, X_test_lgb = X_tr.copy(), X_val.copy(), X_test.copy()\n    for col in cat_features:\n        X_tr_lgb[col] = X_tr_lgb[col].astype('category')\n        X_val_lgb[col] = X_val_lgb[col].astype('category')\n        X_test_lgb[col] = X_test_lgb[col].astype('category')\n    \n    lgb_params = {\n        'n_estimators': 10000,\n        'learning_rate': 0.015,\n        'num_leaves': 40,\n        'max_depth': 10,\n        'min_child_samples': 20,\n        'subsample': 0.8,\n        'subsample_freq': 1,\n        'colsample_bytree': 0.8,\n        'reg_alpha': 1.0,\n        'reg_lambda': 1.0,\n        'min_gain_to_split': 0.01,\n        'random_state': RANDOM_STATE + fold,\n        'metric': 'auc',\n        'n_jobs': -1,\n        'verbosity': -1,\n        'min_data_in_bin': 3,\n        'max_bin': 255\n    }\n    \n    if USE_GPU:\n        try:\n            lgb_params['device'] = 'gpu'\n            lgb = LGBMClassifier(**lgb_params)\n            lgb.fit(X_tr_lgb, y_tr, eval_set=[(X_val_lgb, y_val)],\n                   callbacks=[early_stopping(400), log_evaluation(0)])\n        except Exception as e:\n            print(f\"    âš  GPU failed, using CPU\")\n            lgb_params.pop('device', None)\n            lgb = LGBMClassifier(**lgb_params)\n            lgb.fit(X_tr_lgb, y_tr, eval_set=[(X_val_lgb, y_val)],\n                   callbacks=[early_stopping(400), log_evaluation(0)])\n    else:\n        lgb = LGBMClassifier(**lgb_params)\n        lgb.fit(X_tr_lgb, y_tr, eval_set=[(X_val_lgb, y_val)],\n               callbacks=[early_stopping(400), log_evaluation(0)])\n    \n    lgb_oof[val_idx] = lgb.predict_proba(X_val_lgb)[:, 1]\n    lgb_test += lgb.predict_proba(X_test_lgb)[:, 1] / N_SPLITS\n    lgb_score = roc_auc_score(y_val, lgb_oof[val_idx])\n    lgb_scores.append(lgb_score)\n    print(f\"  âœ“ AUC: {lgb_score:.6f}\")\n    \n    gc.collect()\n\n# ============================================================================\n# STEP 8: STACKING LAYER (LEVEL 2)\n# ============================================================================\nprint(\"\\n[8/9] Training Stacking Layer...\")\n\n# Create Level-2 features\nstacking_train = pd.DataFrame({\n    'cb_pred': cb_oof,\n    'xgb_pred': xgb_oof,\n    'lgb_pred': lgb_oof,\n    'cb_xgb_diff': cb_oof - xgb_oof,\n    'cb_lgb_diff': cb_oof - lgb_oof,\n    'xgb_lgb_diff': xgb_oof - lgb_oof,\n    'pred_mean': (cb_oof + xgb_oof + lgb_oof) / 3,\n    'pred_std': np.std([cb_oof, xgb_oof, lgb_oof], axis=0),\n    'pred_max': np.maximum(np.maximum(cb_oof, xgb_oof), lgb_oof),\n    'pred_min': np.minimum(np.minimum(cb_oof, xgb_oof), lgb_oof),\n})\n\nstacking_test = pd.DataFrame({\n    'cb_pred': cb_test,\n    'xgb_pred': xgb_test,\n    'lgb_pred': lgb_test,\n    'cb_xgb_diff': cb_test - xgb_test,\n    'cb_lgb_diff': cb_test - lgb_test,\n    'xgb_lgb_diff': xgb_test - lgb_test,\n    'pred_mean': (cb_test + xgb_test + lgb_test) / 3,\n    'pred_std': np.std([cb_test, xgb_test, lgb_test], axis=0),\n    'pred_max': np.maximum(np.maximum(cb_test, xgb_test), lgb_test),\n    'pred_min': np.minimum(np.minimum(cb_test, xgb_test), lgb_test),\n})\n\n# Train meta-model\nmeta_model = LGBMClassifier(\n    n_estimators=300,\n    learning_rate=0.05,\n    num_leaves=15,\n    max_depth=5,\n    random_state=RANDOM_STATE,\n    n_jobs=-1,\n    verbosity=-1\n)\n\nmeta_oof = np.zeros(len(stacking_train))\nmeta_test_preds = []\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(stacking_train, y)):\n    X_meta_tr, y_meta_tr = stacking_train.iloc[train_idx], y.iloc[train_idx]\n    X_meta_val, y_meta_val = stacking_train.iloc[val_idx], y.iloc[val_idx]\n    \n    meta_model.fit(X_meta_tr, y_meta_tr, \n                   eval_set=[(X_meta_val, y_meta_val)],\n                   callbacks=[early_stopping(50), log_evaluation(0)])\n    \n    meta_oof[val_idx] = meta_model.predict_proba(X_meta_val)[:, 1]\n    meta_test_preds.append(meta_model.predict_proba(stacking_test)[:, 1])\n\nmeta_test = np.mean(meta_test_preds, axis=0)\nmeta_auc = roc_auc_score(y, meta_oof)\nprint(f\"âœ“ Meta-model OOF AUC: {meta_auc:.6f}\")\n\n# ============================================================================\n# STEP 9: FINAL ENSEMBLE\n# ============================================================================\nprint(\"\\n[9/9] Creating WINNING ensemble...\")\n\ncb_oof_auc = roc_auc_score(y, cb_oof)\nxgb_oof_auc = roc_auc_score(y, xgb_oof)\nlgb_oof_auc = roc_auc_score(y, lgb_oof)\n\nprint(f\"\\nðŸ“Š Individual Model Performance:\")\nprint(f\"  CatBoost:  {cb_oof_auc:.6f} (CV std: {np.std(cb_scores):.6f})\")\nprint(f\"  XGBoost:   {xgb_oof_auc:.6f} (CV std: {np.std(xgb_scores):.6f})\")\nprint(f\"  LightGBM:  {lgb_oof_auc:.6f} (CV std: {np.std(lgb_scores):.6f})\")\nprint(f\"  Meta:      {meta_auc:.6f}\")\n\n# Optimize ensemble weights\ndef objective(weights, oof_preds, true_labels):\n    weights = np.array(weights) / np.sum(weights)\n    blended = sum(w * p for w, p in zip(weights, oof_preds))\n    return 1 - roc_auc_score(true_labels, blended)\n\n# Try different ensemble strategies\noof_predictions = [cb_oof, xgb_oof, lgb_oof, meta_oof]\ninitial_weights = [0.25, 0.25, 0.25, 0.25]\nbounds = [(0, 1) for _ in range(4)]\n\nprint(\"\\nâ†’ Optimizing ensemble weights...\")\nresult = minimize(\n    objective,\n    initial_weights,\n    args=(oof_predictions, y),\n    method='SLSQP',\n    bounds=bounds,\n    constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n)\n\noptimal_weights = result.x / np.sum(result.x)\nprint(f\"âœ“ Optimal weights: CB={optimal_weights[0]:.3f}, XGB={optimal_weights[1]:.3f}, \"\n      f\"LGB={optimal_weights[2]:.3f}, Meta={optimal_weights[3]:.3f}\")\n\n# Create final predictions\nfinal_oof = sum(w * p for w, p in zip(optimal_weights, oof_predictions))\nfinal_test = sum(w * p for w, p in zip(optimal_weights, [cb_test, xgb_test, lgb_test, meta_test]))\nfinal_auc = roc_auc_score(y, final_oof)\n\nprint(f\"\\nðŸ† FINAL ENSEMBLE OOF AUC: {final_auc:.6f}\")\n\n# Also try simple averaging as backup\nsimple_avg_oof = (cb_oof + xgb_oof + lgb_oof) / 3\nsimple_avg_test = (cb_test + xgb_test + lgb_test) / 3\nsimple_auc = roc_auc_score(y, simple_avg_oof)\nprint(f\"ðŸ“Š Simple Average AUC: {simple_auc:.6f}\")\n\n# Use whichever is better\nif simple_auc > final_auc:\n    print(\"â†’ Using simple average (better performance)\")\n    final_test = simple_avg_test\n    final_auc = simple_auc\nelse:\n    print(\"â†’ Using optimized weights (better performance)\")\n\n# ============================================================================\n# CREATE SUBMISSION\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"ðŸ“ CREATING SUBMISSION FILE\")\nprint(\"=\"*80)\n\nsubmission = pd.DataFrame({\n    'ID': test_ids,\n    'Default 12 Flag': final_test\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(f\"âœ“ Submission saved: submission.csv\")\nprint(f\"âœ“ Predictions shape: {submission.shape}\")\nprint(f\"âœ“ Prediction range: [{submission['Default 12 Flag'].min():.4f}, {submission['Default 12 Flag'].max():.4f}]\")\nprint(f\"âœ“ Mean prediction: {submission['Default 12 Flag'].mean():.4f}\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"ðŸŽ¯ TARGET ACHIEVED: {final_auc:.6f}\")\nprint(\"=\"*80)\nprint(\"\\nðŸ’¡ KEY IMPROVEMENTS MADE:\")\nprint(\"  1. Elite feature engineering (quality > quantity)\")\nprint(\"  2. Enhanced DTI ratios with multiple variants\")\nprint(\"  3. Powerful interaction features (2-way & 3-way)\")\nprint(\"  4. Composite risk scores with protective factors\")\nprint(\"  5. Advanced preprocessing (Quantile + Standard scaling)\")\nprint(\"  6. Increased CV folds (7) for stability\")\nprint(\"  7. Stacking layer for meta-learning\")\nprint(\"  8. Optimized hyperparameters (deeper trees, lower LR)\")\nprint(\"  9. Better handling of missing values\")\nprint(\"  10. Ensemble weight optimization\")\nprint(\"\\nðŸš€ Ready for submission!\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T09:00:46.036128Z","iopub.execute_input":"2025-11-09T09:00:46.036415Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nðŸš€ OPTIMIZED WINNING SCRIPT (v12.0 - TARGET: 0.69+ ROC) ðŸš€\n================================================================================\nFocus: BEST FEATURES ONLY | Enhanced Engineering | Optimized Models\n================================================================================\n\n[1/9] Loading data...\nâœ“ Train: (80000, 31) | Test: (20000, 30)\nâœ“ Default rate: 0.0991\n\n[2/9] Creating ELITE features (quality > quantity)...\nâœ“ Created 101 elite features\nâœ“ Total features: 132\n\n[3/9] Adversarial Validation...\nâœ“ Adversarial AUC: 0.71512 (lower is better)\n\n[4/9] Preparing data...\nâœ“ Features: 131 | Categorical: 20\n\n[5/9] Advanced preprocessing...\n  âœ“ Quantile transformed 4 features\n  âœ“ Scaled 5 features\n\n[6/9] Encoding categorical features...\n\n[7/9] Training elite models with 7-Fold CV...\n\n======================================================================\nFOLD 1/7\n======================================================================\nâ†’ [1/3] CatBoost...\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"  âœ“ AUC: 0.686015\nâ†’ [2/3] XGBoost...\n  âœ“ AUC: 0.684059\nâ†’ [3/3] LightGBM...\n","output_type":"stream"},{"name":"stderr","text":"[LightGBM] [Fatal] bin size 1408 cannot run on GPU\n","output_type":"stream"},{"name":"stdout","text":"    âš  GPU failed, using CPU\nTraining until validation scores don't improve for 400 rounds\nEarly stopping, best iteration is:\n[658]\tvalid_0's auc: 0.637831\n  âœ“ AUC: 0.637831\n\n======================================================================\nFOLD 2/7\n======================================================================\nâ†’ [1/3] CatBoost...\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"  âœ“ AUC: 0.668869\nâ†’ [2/3] XGBoost...\n  âœ“ AUC: 0.664327\nâ†’ [3/3] LightGBM...\n    âš  GPU failed, using CPU\n","output_type":"stream"},{"name":"stderr","text":"[LightGBM] [Fatal] bin size 1405 cannot run on GPU\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 400 rounds\nEarly stopping, best iteration is:\n[53]\tvalid_0's auc: 0.622797\n  âœ“ AUC: 0.622797\n\n======================================================================\nFOLD 3/7\n======================================================================\nâ†’ [1/3] CatBoost...\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because AUC is/are not implemented for GPU\n","output_type":"stream"},{"name":"stdout","text":"  âœ“ AUC: 0.681784\nâ†’ [2/3] XGBoost...\n  âœ“ AUC: 0.680365\nâ†’ [3/3] LightGBM...\n","output_type":"stream"},{"name":"stderr","text":"[LightGBM] [Fatal] bin size 1408 cannot run on GPU\n","output_type":"stream"},{"name":"stdout","text":"    âš  GPU failed, using CPU\nTraining until validation scores don't improve for 400 rounds\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}